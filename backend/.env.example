# Server Configuration
BACKEND_PORT=5001
NODE_ENV=development

# CORS Configuration
# IMPORTANT: Set these in production to secure your API
# For local development:
FRONTEND_URL=http://localhost:5173
# For production (Render deployment), set this to your Vercel frontend URL:
# FRONTEND_URL=https://your-app.vercel.app
# or use CORS_ORIGIN as an alternative
CORS_ORIGIN=http://localhost:5173

# Session Configuration
# CRITICAL: Change this in production!
SESSION_SECRET=your-secret-key-change-in-production

# LLM Provider Configuration
# Supported providers: lm-studio, openai, anthropic, gemini, custom
LLM_PROVIDER=lm-studio

# LM Studio Configuration (default provider)
LM_STUDIO_URL=http://127.0.0.1:1234
LLM_MODEL_NAME=local-model

# OpenAI Configuration (if using openai provider)
#OPENAI_API_KEY=your-openai-api-key
#OPENAI_BASE_URL=https://api.openai.com/v1
#OPENAI_MODEL=gpt-4o-mini

# Anthropic Configuration (if using anthropic provider)
#ANTHROPIC_API_KEY=your-anthropic-api-key
#ANTHROPIC_BASE_URL=https://api.anthropic.com/v1
#ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Gemini Configuration (if using gemini provider)
#GEMINI_API_KEY=your-gemini-api-key
#GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
#GEMINI_MODEL=gemini-1.5-flash

# Custom Provider Configuration
#CUSTOM_BASE_URL=your-custom-llm-url
#CUSTOM_API_KEY=your-custom-api-key
#CUSTOM_MODEL=your-model-name

# LLM Parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=500

# Logging Configuration
LOG_LEVEL=info

# Authentication Configuration
# Leave empty to disable authentication
JWT_SECRET=your-jwt-secret-change-in-production
ACCESS_PASSWORD=
INVITE_CODES=

# Example authentication setup:
# JWT_SECRET=my-super-secret-jwt-key-change-this
# ACCESS_PASSWORD=my-strong-password
# INVITE_CODES=CODE123,CODE456,CODE789
