# éƒ¨ç½²æŒ‡å— - Deployment Guide

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•å°†"é“æ­‰åŠ©æ‰‹"åº”ç”¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒã€‚

---

## ğŸ“‹ ç›®å½•

1. [æ¶æ„æ¦‚è¿°](#æ¶æ„æ¦‚è¿°)
2. [Verceléƒ¨ç½²æ–¹æ¡ˆï¼ˆæ¨èï¼‰](#verceléƒ¨ç½²æ–¹æ¡ˆæ¨è)
3. [å…¶ä»–éƒ¨ç½²æ–¹æ¡ˆ](#å…¶ä»–éƒ¨ç½²æ–¹æ¡ˆ)
4. [ç¯å¢ƒå˜é‡é…ç½®](#ç¯å¢ƒå˜é‡é…ç½®)
5. [éƒ¨ç½²æ£€æŸ¥æ¸…å•](#éƒ¨ç½²æ£€æŸ¥æ¸…å•)

---

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

### åº”ç”¨ç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â”€â–¶â”‚  LLM Service â”‚
â”‚   (React)   â”‚     â”‚  (Node.js)  â”‚     â”‚  (External)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Vercel          Railway/Render       OpenAI/Anthropic
                     or Serverless         /Gemini/ç­‰
```

### å…³é”®ç‰¹æ€§

- **å‰ç«¯**: é™æ€Reactåº”ç”¨ï¼Œé€‚åˆCDNéƒ¨ç½²
- **åç«¯**: Node.js ExpressæœåŠ¡ï¼Œéœ€è¦æŒç»­è¿è¡Œæˆ–Serverless
- **LLM**: å¯ä»¥æ˜¯æœ¬åœ°ï¼ˆLM Studioï¼‰æˆ–äº‘ç«¯APIï¼ˆOpenAIç­‰ï¼‰

---

## âœ… Verceléƒ¨ç½²æ–¹æ¡ˆï¼ˆæ¨èï¼‰

### æ–¹æ¡ˆAï¼šå‰ç«¯åœ¨Vercel + åç«¯åœ¨å…¶ä»–å¹³å°ï¼ˆæ¨èâ˜…â˜…â˜…â˜…â˜…ï¼‰

**è¿™æ˜¯ä¸šç•Œæœ€ä½³å®è·µï¼ŒåŸå› ï¼š**
- å‰ç«¯é™æ€èµ„æºä½¿ç”¨CDNï¼ˆå¿«é€Ÿã€ä¾¿å®œï¼‰
- åç«¯é•¿æ—¶é—´è¿è¡ŒæœåŠ¡ç‹¬ç«‹éƒ¨ç½²ï¼ˆç¨³å®šã€æ— è¶…æ—¶é™åˆ¶ï¼‰
- å„ç»„ä»¶ç‹¬ç«‹æ‰©å±•ï¼Œäº’ä¸å½±å“

#### æ­¥éª¤1ï¼šå‰ç«¯éƒ¨ç½²åˆ°Vercel

```bash
# 1. è¿æ¥GitHubä»“åº“åˆ°Vercel
# 2. åœ¨Vercelé…ç½®ï¼š
Root Directory: frontend
Framework Preset: Vite
Build Command: npm run build
Output Directory: dist

# 3. ç¯å¢ƒå˜é‡ï¼š
VITE_API_URL=https://your-backend.railway.app
```

#### æ­¥éª¤2ï¼šåç«¯éƒ¨ç½²åˆ°Railway/Renderï¼ˆæ¨èï¼‰

**Railwayéƒ¨ç½²ï¼ˆæœ€ç®€å•ï¼‰ï¼š**

1. è®¿é—® [Railway.app](https://railway.app)
2. è¿æ¥GitHubä»“åº“
3. é€‰æ‹© `backend` ç›®å½•
4. æ·»åŠ ç¯å¢ƒå˜é‡ï¼ˆè§ä¸‹æ–¹ï¼‰
5. è‡ªåŠ¨éƒ¨ç½²å®Œæˆ

**ç¯å¢ƒå˜é‡é…ç½®ï¼š**
```bash
# å¿…éœ€
PORT=5001
NODE_ENV=production

# LLMé…ç½®ï¼ˆä½¿ç”¨äº‘ç«¯APIï¼‰
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-xxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini

# æˆ–ä½¿ç”¨Anthropic
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxx

# è®¤è¯ï¼ˆå¯é€‰ï¼‰
JWT_SECRET=your-production-secret
ACCESS_PASSWORD=your-secure-password
# INVITE_CODES=CODE1,CODE2,CODE3

# CORSï¼ˆé‡è¦ï¼ï¼‰
FRONTEND_URL=https://your-app.vercel.app
CORS_ORIGIN=https://your-app.vercel.app
```

**Railwayéƒ¨ç½²å‘½ä»¤ï¼š**
```json
// railway.json (åœ¨backendç›®å½•)
{
  "$schema": "https://railway.app/railway.schema.json",
  "build": {
    "builder": "NIXPACKS",
    "buildCommand": "npm install && npm run build"
  },
  "deploy": {
    "startCommand": "npm start",
    "restartPolicyType": "ON_FAILURE"
  }
}
```

---

### æ–¹æ¡ˆBï¼šå…¨éƒ¨åœ¨Vercelï¼ˆServerless Functionsï¼‰

**ä¼˜ç‚¹ï¼š**
- å•ä¸€å¹³å°ç®¡ç†
- è‡ªåŠ¨æ‰©å±•
- æŒ‰ä½¿ç”¨ä»˜è´¹

**ç¼ºç‚¹ï¼š**
- Serverlesså‡½æ•°æœ‰10ç§’è¶…æ—¶é™åˆ¶
- LLMå“åº”å¯èƒ½è¶…æ—¶
- æ— çŠ¶æ€ï¼Œéœ€è¦å¤–éƒ¨æ•°æ®åº“

**é€‚ç”¨åœºæ™¯ï¼š** ä½¿ç”¨å¿«é€Ÿå“åº”çš„LLM APIï¼ˆOpenAI Turboã€Gemini Flashç­‰ï¼‰

#### Vercel Serverlessé…ç½®

éœ€è¦å°†Expressåº”ç”¨æ”¹é€ ä¸ºServerlesså‡½æ•°ï¼š

```javascript
// api/index.js (Vercel Serverless Entry)
import express from 'express';
import chatRoutes from '../backend/src/routes/chat.routes.js';
import healthRoutes from '../backend/src/routes/health.routes.js';
import authRoutes from '../backend/src/routes/auth.routes.js';
// ... å…¶ä»–imports

const app = express();

// Middleware
app.use(express.json());
app.use(cors({
  origin: process.env.FRONTEND_URL,
  credentials: true
}));

// Routes
app.use('/api/auth', authRoutes);
app.use('/api/health', healthRoutes);
app.use('/api/chat', chatRoutes);

// Export as Vercel Serverless Function
export default app;
```

**vercel.jsoné…ç½®ï¼š**
```json
{
  "version": 2,
  "builds": [
    {
      "src": "frontend/package.json",
      "use": "@vercel/static-build",
      "config": {
        "distDir": "dist"
      }
    },
    {
      "src": "api/index.js",
      "use": "@vercel/node"
    }
  ],
  "routes": [
    {
      "src": "/api/(.*)",
      "dest": "/api/index.js"
    },
    {
      "src": "/(.*)",
      "dest": "/frontend/$1"
    }
  ],
  "env": {
    "LLM_PROVIDER": "@llm-provider",
    "OPENAI_API_KEY": "@openai-api-key"
  }
}
```

**é™åˆ¶ï¼š**
- âš ï¸ æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼š10ç§’ï¼ˆHobbyï¼‰æˆ–60ç§’ï¼ˆProï¼‰
- âš ï¸ LLMå¿…é¡»å¿«é€Ÿå“åº”
- âš ï¸ ä¸é€‚åˆæœ¬åœ°LM Studio

---

## ğŸŒ å…¶ä»–éƒ¨ç½²æ–¹æ¡ˆ

### æ–¹æ¡ˆCï¼šä¼ ç»ŸVPSéƒ¨ç½²ï¼ˆAWS/DigitalOcean/Linodeï¼‰

**ä¼˜ç‚¹ï¼š**
- å®Œå…¨æ§åˆ¶
- å¯è¿è¡Œæœ¬åœ°LM Studio
- æ— è¶…æ—¶é™åˆ¶

**æ­¥éª¤ï¼š**
```bash
# 1. å‡†å¤‡æœåŠ¡å™¨ï¼ˆUbuntu 22.04ï¼‰
sudo apt update && sudo apt upgrade -y
sudo apt install -y nodejs npm nginx

# 2. å…‹éš†ä»£ç 
git clone https://github.com/your-repo/apologize-is-all-you-need.git
cd apologize-is-all-you-need

# 3. å®‰è£…ä¾èµ–
cd backend && npm install && npm run build
cd ../frontend && npm install && npm run build

# 4. ä½¿ç”¨PM2ç®¡ç†è¿›ç¨‹
npm install -g pm2

# å¯åŠ¨åç«¯
cd backend
pm2 start dist/server.js --name apologize-backend

# 5. Nginxé…ç½®
sudo nano /etc/nginx/sites-available/apologize
```

**Nginxé…ç½®ï¼š**
```nginx
server {
    listen 80;
    server_name your-domain.com;

    # å‰ç«¯é™æ€æ–‡ä»¶
    location / {
        root /path/to/frontend/dist;
        try_files $uri $uri/ /index.html;
    }

    # åç«¯APIä»£ç†
    location /api {
        proxy_pass http://localhost:5001;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

---

### æ–¹æ¡ˆDï¼šDockeréƒ¨ç½²

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "5001:5001"
    environment:
      - NODE_ENV=production
      - LLM_PROVIDER=${LLM_PROVIDER}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped
```

---

## ğŸ”§ ç¯å¢ƒå˜é‡é…ç½®

### å‰ç«¯ç¯å¢ƒå˜é‡

```bash
# .env (frontend/)
VITE_API_URL=https://your-backend-url.com
```

### åç«¯ç¯å¢ƒå˜é‡ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

```bash
# .env (backend/)

# Server
NODE_ENV=production
BACKEND_PORT=5001
FRONTEND_URL=https://your-frontend.vercel.app

# LLM - OpenAI
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=500

# æˆ– LLM - Anthropic
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxx
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# æˆ– LLM - Gemini
# LLM_PROVIDER=gemini
# GEMINI_API_KEY=AIxxxxxxxxxxxxxxxxxxxx
# GEMINI_MODEL=gemini-1.5-flash

# Authentication
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production
ACCESS_PASSWORD=your-secure-password
# INVITE_CODES=CODE123,CODE456

# Logging
LOG_LEVEL=info

# Session
SESSION_SECRET=your-session-secret-change-this

# CORS
CORS_ORIGIN=https://your-frontend.vercel.app
```

---

## âœ… éƒ¨ç½²æ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰

- [ ] æ‰€æœ‰ç¯å¢ƒå˜é‡å·²é…ç½®
- [ ] JWT_SECRETå’ŒSESSION_SECRETå·²æ›´æ”¹ä¸ºå¼ºå¯†ç 
- [ ] LLM APIå¯†é’¥å·²é…ç½®
- [ ] å‰ç«¯API_URLæŒ‡å‘æ­£ç¡®çš„åç«¯åœ°å€
- [ ] CORSé…ç½®åŒ…å«å‰ç«¯åŸŸå

### éƒ¨ç½²å

- [ ] å‰ç«¯å¯ä»¥è®¿é—®å¹¶åŠ è½½
- [ ] å¥åº·æ£€æŸ¥ç«¯ç‚¹æ­£å¸¸ï¼š`/api/health`
- [ ] LLMå¥åº·æ£€æŸ¥æ­£å¸¸ï¼š`/api/health/llm`
- [ ] å¯ä»¥æˆåŠŸå‘é€æ¶ˆæ¯å¹¶è·å¾—å›å¤
- [ ] è®¤è¯åŠŸèƒ½æ­£å¸¸ï¼ˆå¦‚æœå¯ç”¨ï¼‰
- [ ] æ—¥å¿—è®°å½•æ­£å¸¸
- [ ] HTTPSè¯ä¹¦é…ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒå¿…éœ€ï¼‰

### ç›‘æ§

- [ ] è®¾ç½®åç«¯æœåŠ¡ç›‘æ§ï¼ˆUptimeRobotç­‰ï¼‰
- [ ] é…ç½®é”™è¯¯å‘Šè­¦
- [ ] ç›‘æ§APIå“åº”æ—¶é—´
- [ ] ç›‘æ§LLM APIä½¿ç”¨é‡å’Œæˆæœ¬

---

## ğŸ¯ æ¨èæ–¹æ¡ˆæ€»ç»“

### ä¸ªäººé¡¹ç›®/å°å›¢é˜Ÿï¼ˆæ¨èï¼‰

```
Frontend: Vercel (å…è´¹)
Backend:  Railway (å…è´¹tieræˆ–$5/æœˆ)
LLM:      OpenAI API ($0.15/1M tokens) æˆ– Gemini API (å…è´¹tier)
æ•°æ®åº“:   Railway PostgreSQL (å¦‚éœ€è¦)
```

**æœˆæˆæœ¬ä¼°ç®—ï¼š**
- Vercel: $0
- Railway: $5
- OpenAI API: ~$1-10ï¼ˆæ ¹æ®ä½¿ç”¨é‡ï¼‰
- **æ€»è®¡ï¼š$6-15/æœˆ**

### ä¼ä¸š/é«˜æµé‡

```
Frontend: Vercel Pro ($20/æœˆ)
Backend:  AWS ECS/Fargate æˆ– Railway Pro
LLM:      OpenAI API + ç¼“å­˜å±‚
CDN:      Cloudflare (å…è´¹)
ç›‘æ§:     Datadog/NewRelic
```

---

## ğŸ”— ç›¸å…³èµ„æº

- [Verceléƒ¨ç½²æ–‡æ¡£](https://vercel.com/docs)
- [Railwayéƒ¨ç½²æ–‡æ¡£](https://docs.railway.app)
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs)
- [Nginxé…ç½®æŒ‡å—](https://nginx.org/en/docs/)

---

## ğŸ’¡ å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆä¸æ¨èå…¨éƒ¨ç”¨Vercel Serverlessï¼Ÿ**
A: Serverlessæœ‰10-60ç§’è¶…æ—¶é™åˆ¶ï¼ŒLLMå“åº”å¯èƒ½è¶…æ—¶ã€‚åˆ†ç¦»éƒ¨ç½²æ›´ç¨³å®šã€‚

**Q: å¯ä»¥ä½¿ç”¨æœ¬åœ°LM Studioéƒ¨ç½²å—ï¼Ÿ**
A: å¯ä»¥ï¼Œä½†éœ€è¦VPSæˆ–å®¶åº­æœåŠ¡å™¨ï¼Œä¸èƒ½ç”¨Serverlessã€‚

**Q: å¦‚ä½•é™ä½LLM APIæˆæœ¬ï¼Ÿ**
A: ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹ï¼ˆå¦‚GPT-4o-miniï¼‰ï¼Œæ·»åŠ å“åº”ç¼“å­˜ï¼Œé™åˆ¶tokenæ•°é‡ã€‚

**Q: HTTPSå¦‚ä½•é…ç½®ï¼Ÿ**
A: Vercelè‡ªåŠ¨æä¾›HTTPSã€‚Railwayä¹Ÿè‡ªåŠ¨é…ç½®ã€‚VPSéœ€è¦Let's Encryptã€‚

---

**æœ€åæ›´æ–°**: 2025-11-15
